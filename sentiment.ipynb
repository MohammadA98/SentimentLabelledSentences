{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas matplotlib seaborn  nltk textblob vaderSentiment wordcloud\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text Processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Display Formatting\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset extraction and organization\n",
    "### Load and combine all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have to jiggle the plug to get it to line up...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>If you have several dozen or several hundred c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>If you are Razr owner...you must have this!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Needless to say, I wasted my money.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What a waste of money and time!.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review Sentiment\n",
       "0  So there is no way for me to plug it in here i...       0  \n",
       "1                        Good case, Excellent value.       1  \n",
       "2                             Great for the jawbone.       1  \n",
       "3  Tied to charger for conversations lasting more...       0  \n",
       "4                                  The mic is great.       1  \n",
       "5  I have to jiggle the plug to get it to line up...       0  \n",
       "6  If you have several dozen or several hundred c...       0  \n",
       "7        If you are Razr owner...you must have this!       1  \n",
       "8                Needless to say, I wasted my money.       0  \n",
       "9                   What a waste of money and time!.       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd # for dataframes\n",
    "from IPython.display import display # for display\n",
    "\n",
    "# Load datasets\n",
    "dataset1 = pd.read_csv('data/amazon_cells_labelled.txt', delimiter='\\t', header=None, names=['Review', 'Sentiment'])\n",
    "dataset2 = pd.read_csv('data/imdb_labelled.txt', delimiter='\\t', header=None, names=['Review', 'Sentiment'])\n",
    "dataset3 = pd.read_csv('data/yelp_labelled.txt', delimiter='\\t', header=None, names=['Review', 'Sentiment'])\n",
    "\n",
    "# Combine datasets\n",
    "combined_dataset = pd.concat([dataset1, dataset2, dataset3], ignore_index=True)\n",
    "\n",
    "# Ensure correct column ordering\n",
    "combined_dataset = combined_dataset[['Review', 'Sentiment']]\n",
    "\n",
    "# Convert Sentiment column to a centered string format \n",
    "combined_dataset[\"Sentiment\"] = combined_dataset[\"Sentiment\"].astype(str).apply(lambda x: f\"{x:^5}\")\n",
    "\n",
    "# Display first 10 rows\n",
    "display(combined_dataset.head(10)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review       0\n",
      "Sentiment    0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\azizi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\azizi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download required nltk data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Removing duplicates\n",
    "combined_dataset = combined_dataset.drop_duplicates(subset=['Review'])\n",
    "\n",
    "# Checking for missing values\n",
    "print(combined_dataset.isnull().sum().to_string())\n",
    "\n",
    "# Converting to lowercase\n",
    "combined_dataset['Review'] = combined_dataset['Review'].str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contractions dictionary\n",
    "contractions_dict = {\n",
    "    \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \n",
    "    \"it's\": \"it is\", \"i'm\": \"i am\", \"you're\": \"you are\", \n",
    "    \"they're\": \"they are\", \"we're\": \"we are\", \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\", \"that's\": \"that is\", \"what's\": \"what is\", \n",
    "    \"there's\": \"there is\", \"isn't\": \"is not\", \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\", \"weren't\": \"were not\", \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\", \"didn't\": \"did not\", \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\", \"hadn't\": \"had not\", \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\", \"couldn't\": \"could not\", \"mustn't\": \"must not\",\n",
    "    \"let's\": \"let us\", \"i've\": \"i have\", \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\", \"they've\": \"they have\"\n",
    "}\n",
    "\n",
    "def normalize_contractions(text):\n",
    "    if isinstance(text, str):  \n",
    "        words = text.split()\n",
    "        expanded_words = [contractions_dict[word.lower()] if word.lower() in contractions_dict else word for word in words]\n",
    "        return \" \".join(expanded_words)\n",
    "    return text  \n",
    "\n",
    "combined_dataset['Review'] = combined_dataset['Review'].apply(normalize_contractions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuation, Special Characters, and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using `.translate()` for fast standard punctuation removal\n",
    "combined_dataset['Review'] = combined_dataset['Review'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# using `.str.replace()` to remove any remaining Unicode punctuation\n",
    "combined_dataset['Review'] = combined_dataset['Review'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# Removing special characters & numbers\n",
    "combined_dataset['Review'] = combined_dataset['Review'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "\n",
    "# Removing non-ASCII characters\n",
    "combined_dataset['Review'] = combined_dataset['Review'].apply(lambda x: x.encode('ascii', 'ignore').decode('utf-8') if isinstance(x, str) else x)\n",
    "\n",
    "# Removing special Unicode symbols\n",
    "combined_dataset['Review'] = combined_dataset['Review'].str.replace(r'[“”‘’]', '', regex=True)\n",
    "\n",
    "# Normalizing spaces\n",
    "combined_dataset['Review'] = combined_dataset['Review'].str.strip().replace(r'\\s+', ' ', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Correct Spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    if isinstance(text, str):\n",
    "        return str(TextBlob(text).correct())  \n",
    "    return text  \n",
    "\n",
    "combined_dataset['Review'] = combined_dataset['Review'].apply(correct_spelling)\n",
    "\n",
    "\n",
    "# Faster but less accurate\n",
    "# from spellchecker import SpellChecker\n",
    "\n",
    "# spell = SpellChecker()\n",
    "\n",
    "# def fast_correct_spelling(text):\n",
    "#     if isinstance(text, str):\n",
    "#         words = text.split()\n",
    "#         corrected_words = spell.correction(words)  # Batch correct all words at once\n",
    "#         return \" \".join(corrected_words)\n",
    "#     return text\n",
    "\n",
    "\n",
    "# combined_dataset['Review'] = combined_dataset['Review'].apply(fast_correct_spelling) # Apply spell correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        return \" \".join(lemmatized_words)\n",
    "    return text  \n",
    "\n",
    "combined_dataset['Review'] = combined_dataset['Review'].apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set([ \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\",\n",
    "    \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\",\n",
    "    \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\",\n",
    "    \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\",\n",
    "    \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\",\n",
    "    \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n",
    "    \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\",\n",
    "    \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\",\n",
    "    \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \n",
    "    \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\",\n",
    "    \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"])\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    refined_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word not in stopwords_set:\n",
    "            refined_words.append(word)\n",
    "    \n",
    "    cleaned_text = ' '.join(refined_words)\n",
    "    return cleaned_text\n",
    "\n",
    "combined_dataset['Review'] = combined_dataset['Review'].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# IT seems that the stopwords are not removed completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "### Visualizing and analyzing sentiment distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use bar charts \n",
    ">to show positive vs. negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of review lengths\n",
    ">Histogram showing text lenght distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud for Most Frequent Word\n",
    ">Generating word clouds for both positive and negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency Analysis\n",
    ">Identifing most common words in positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Vader\n",
    " >Leveraging a rule-based model to assess sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic Regression \n",
    ">Training a supervised learning model for sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold-Based Accuracy\n",
    ">Measures how often VADER’s sentiment matches labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE) \n",
    ">Measures how close VADER's scores are to actual sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spearman/Pearson Correlation \n",
    ">Measures how well VADER scores align with actual sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    ">Essential for analyzing false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve & AUC\n",
    ">Measures classification performance across thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader vs Logistic Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aai_a500",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
